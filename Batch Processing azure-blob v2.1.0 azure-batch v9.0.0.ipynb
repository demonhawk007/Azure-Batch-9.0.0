{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import datetime as date\n",
    "from datetime import datetime, timedelta\n",
    "import io\n",
    "import os,uuid\n",
    "import sys\n",
    "import time\n",
    "import config\n",
    "try:\n",
    "    input = raw_input\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.storage.blob as azureblob\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import azure.batch.models as batchmodels\n",
    "import azure.batch.batch_auth as batch_auth\n",
    "import azure.batch._batch_service_client as batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('.')\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_yes_no(question, default=\"yes\"):\n",
    "    \"\"\"\n",
    "    Prompts the user for yes/no input, displaying the specified question text.\n",
    "\n",
    "    :param str question: The text of the prompt for input.\n",
    "    :param str default: The default if the user hits <ENTER>. Acceptable values\n",
    "    are 'yes', 'no', and None.\n",
    "    :rtype: str\n",
    "    :return: 'yes' or 'no'\n",
    "    \"\"\"\n",
    "    valid = {'y': 'yes', 'n': 'no'}\n",
    "    if default is None:\n",
    "        prompt = ' [y/n] '\n",
    "    elif default == 'yes':\n",
    "        prompt = ' [Y/n] '\n",
    "    elif default == 'no':\n",
    "        prompt = ' [y/N] '\n",
    "    else:\n",
    "        raise ValueError(\"Invalid default answer: '{}'\".format(default))\n",
    "\n",
    "    while 1:\n",
    "        choice = input(question + prompt).lower()\n",
    "        if default and not choice:\n",
    "            return default\n",
    "        try:\n",
    "            return valid[choice[0]]\n",
    "        except (KeyError, IndexError):\n",
    "            print(\"Please respond with 'yes' or 'no' (or 'y' or 'n').\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_batch_exception(batch_exception):\n",
    "    \"\"\"\n",
    "    Prints the contents of the specified Batch exception.\n",
    "\n",
    "    :param batch_exception:\n",
    "    \"\"\"\n",
    "    print('-------------------------------------------')\n",
    "    print('Exception encountered:')\n",
    "    if batch_exception.error and \\\n",
    "            batch_exception.error.message and \\\n",
    "            batch_exception.error.message.value:\n",
    "        print(batch_exception.error.message.value)\n",
    "        if batch_exception.error.values:\n",
    "            print()\n",
    "            for mesg in batch_exception.error.values:\n",
    "                print('{}:\\t{}'.format(mesg.key, mesg.value))\n",
    "    print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_to_container(block_blob_client, container_name, file_path):\n",
    "    \"\"\"\n",
    "    Uploads a local file to an Azure Blob storage container.\n",
    "\n",
    "    :param block_blob_client: A blob service client.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str container_name: The name of the Azure Blob storage container.\n",
    "    :param str file_path: The local path to the file.\n",
    "    :rtype: `azure.batch.models.ResourceFile`\n",
    "    :return: A ResourceFile initialized with a SAS URL appropriate for Batch\n",
    "    tasks.\n",
    "    \"\"\"\n",
    "    blob_name = os.path.basename(file_path)\n",
    "\n",
    "    print('Uploading file {} to container [{}]...'.format(file_path,\n",
    "                                                          container_name))\n",
    "\n",
    "    block_blob_client.create_blob_from_path(container_name,\n",
    "                                            blob_name,\n",
    "                                            file_path)\n",
    "\n",
    "    sas_token = block_blob_client.generate_blob_shared_access_signature(\n",
    "        container_name,\n",
    "        blob_name,\n",
    "        permission=azureblob.BlobPermissions.READ,\n",
    "        expiry=datetime.utcnow() + timedelta(hours=2))\n",
    "\n",
    "    sas_url = block_blob_client.make_blob_url(container_name,\n",
    "                                              blob_name,\n",
    "                                              sas_token=sas_token)\n",
    "\n",
    "    return batchmodels.ResourceFile(http_url=sas_url, file_path=blob_name)\n",
    "#     return ResourceFile(http_url=sas_url, file_path=blob_name) #Changes made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_container_sas_token(block_blob_client,\n",
    "                            container_name, blob_permissions):\n",
    "    \"\"\"\n",
    "    Obtains a shared access signature granting the specified permissions to the\n",
    "    container.\n",
    "\n",
    "    :param block_blob_client: A blob service client.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str container_name: The name of the Azure Blob storage container.\n",
    "    :param BlobPermissions blob_permissions:\n",
    "    :rtype: str\n",
    "    :return: A SAS token granting the specified permissions to the container.\n",
    "    \"\"\"\n",
    "    # Obtain the SAS token for the container, setting the expiry time and\n",
    "    # permissions. In this case, no start time is specified, so the shared\n",
    "    # access signature becomes valid immediately.\n",
    "    container_sas_token = \\\n",
    "        block_blob_client.generate_container_shared_access_signature(\n",
    "            container_name,\n",
    "            permission=blob_permissions,\n",
    "            expiry=datetime.utcnow() + timedelta(hours=2))\n",
    "\n",
    "    return container_sas_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please set the value of node here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pool(batch_service_client, pool_id):\n",
    "    \"\"\"\n",
    "    Creates a pool of compute nodes with the specified OS settings.\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str pool_id: An ID for the new pool.\n",
    "    :param str publisher: Marketplace image publisher\n",
    "    :param str offer: Marketplace image offer\n",
    "    :param str sku: Marketplace image sku\n",
    "    \"\"\"\n",
    "    print('Creating pool [{}]...'.format(pool_id))\n",
    "\n",
    "    # Create a new pool of Linux compute nodes using an Azure Virtual Machines\n",
    "    # Marketplace image. For more information about creating pools of Linux\n",
    "    # nodes, see:\n",
    "    # https://azure.microsoft.com/documentation/articles/batch-linux-nodes/\n",
    "    new_pool = batch.models.PoolAddParameter(\n",
    "        id=pool_id,\n",
    "        virtual_machine_configuration=batchmodels.VirtualMachineConfiguration(\n",
    "            image_reference=batchmodels.ImageReference(\n",
    "                publisher=\"Canonical\",\n",
    "                offer=\"UbuntuServer\",\n",
    "                sku=\"18.04-LTS\",\n",
    "                version=\"latest\"\n",
    "            ),\n",
    "            node_agent_sku_id=\"batch.node.ubuntu 18.04\"),\n",
    "        vm_size=config._POOL_VM_SIZE,\n",
    "        target_dedicated_nodes=0,\n",
    "        target_low_priority_nodes=0\n",
    "#         target_low_priority_nodes=config._POOL_NODE_COUNT\n",
    "    )\n",
    "    batch_service_client.pool.add(new_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job(batch_service_client, job_id, pool_id):\n",
    "    \"\"\"\n",
    "    Creates a job with the specified ID, associated with the specified pool.\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str job_id: The ID for the job.\n",
    "    :param str pool_id: The ID for the pool.\n",
    "    \"\"\"\n",
    "    print('Creating job [{}]...'.format(job_id))\n",
    "\n",
    "    job = batch.models.JobAddParameter(\n",
    "        id=job_id,\n",
    "        pool_info=batch.models.PoolInformation(pool_id=pool_id))\n",
    "\n",
    "    batch_service_client.job.add(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tasks(batch_service_client, job_id, input_files):\n",
    "    \"\"\"\n",
    "    Adds a task for each input file in the collection to the specified job.\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str job_id: The ID of the job to which to add the tasks.\n",
    "    :param list input_files: A collection of input files. One task will be\n",
    "     created for each input file.\n",
    "    :param output_container_sas_token: A SAS token granting write access to\n",
    "    the specified Azure Blob storage container.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Adding {} tasks to job [{}]...'.format(len(input_files), job_id))\n",
    "\n",
    "    tasks = list()\n",
    "\n",
    "    for idx, input_file in enumerate(input_files):\n",
    "        command = \"/bin/bash -c \\\"cat {}\\\"\".format(input_file.file_path)\n",
    "        tasks.append(batch.models.TaskAddParameter(\n",
    "            id='Task{}'.format(idx),\n",
    "            command_line=command,\n",
    "            resource_files=[input_file]\n",
    "        ))\n",
    "    batch_service_client.task.add_collection(job_id, tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_tasks_to_complete(batch_service_client, job_id, timeout):\n",
    "    \"\"\"\n",
    "    Returns when all tasks in the specified job reach the Completed state.\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str job_id: The id of the job whose tasks should be to monitored.\n",
    "    :param timedelta timeout: The duration to wait for task completion. If all\n",
    "    tasks in the specified job do not reach Completed state within this time\n",
    "    period, an exception will be raised.\n",
    "    \"\"\"\n",
    "    timeout_expiration = datetime.now() + timeout\n",
    "\n",
    "    print(\"Monitoring all tasks for 'Completed' state, timeout in {}...\"\n",
    "          .format(timeout), end='')\n",
    "\n",
    "    while datetime.now() < timeout_expiration:\n",
    "        print('.', end='')\n",
    "        sys.stdout.flush()\n",
    "        tasks = batch_service_client.task.list(job_id)\n",
    "\n",
    "        incomplete_tasks = [task for task in tasks if\n",
    "                            task.state != batchmodels.TaskState.completed]\n",
    "        if not incomplete_tasks:\n",
    "            print()\n",
    "            return True\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "\n",
    "    print()\n",
    "    print(f'timeout_expiration:{timeout_expiration}')\n",
    "    raise RuntimeError(\"ERROR: Tasks did not reach 'Completed' state within \"\n",
    "                       \"timeout period of \" + str(timeout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_task_output(batch_service_client, job_id, encoding=None):\n",
    "    \"\"\"Prints the stdout.txt file for each task in the job.\n",
    "    :param batch_client: The batch client to use.\n",
    "    :type batch_client: `batchserviceclient.BatchServiceClient`\n",
    "    :param str job_id: The id of the job with task output files to print.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Printing task output...')\n",
    "\n",
    "    tasks = batch_service_client.task.list(job_id)\n",
    "    print(f'tasks:{tasks}')\n",
    "\n",
    "    for task in tasks:\n",
    "\n",
    "        node_id = batch_service_client.task.get(\n",
    "            job_id, task.id).node_info.node_id\n",
    "        print(\"Task: {}\".format(task.id))\n",
    "        print(\"Node: {}\".format(node_id))\n",
    "\n",
    "        stream = batch_service_client.file.get_from_task(\n",
    "            job_id, task.id, config._STANDARD_OUT_FILE_NAME)\n",
    "\n",
    "        file_text = _read_stream_as_string(\n",
    "            stream,\n",
    "            encoding)\n",
    "        print(\"Standard output:\")\n",
    "        print(file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_stream_as_string(stream, encoding):\n",
    "    \"\"\"Read stream as string\n",
    "    :param stream: input stream generator\n",
    "    :param str encoding: The encoding of the file. The default is utf-8.\n",
    "    :return: The file content.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    output = io.BytesIO()\n",
    "    try:\n",
    "        for data in stream:\n",
    "            output.write(data)\n",
    "        if encoding is None:\n",
    "            encoding = 'utf-8'\n",
    "        return output.getvalue().decode(encoding)\n",
    "    finally:\n",
    "        output.close()\n",
    "    raise RuntimeError('could not write data to stream or decode bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample start: 2020-07-29 00:24:01\n",
      "\n",
      "Uploading file /home/fission/Desktop/Azure_batch/Practice/taskdata0.txt to container [input]...\n",
      "Uploading file /home/fission/Desktop/Azure_batch/Practice/taskdata1.txt to container [input]...\n",
      "Uploading file /home/fission/Desktop/Azure_batch/Practice/taskdata2.txt to container [input]...\n",
      "Creating pool [PythonQuickstartPool]...\n",
      "Creating job [PythonQuickstartJob]...\n",
      "Adding 3 tasks to job [PythonQuickstartJob]...\n",
      "Monitoring all tasks for 'Completed' state, timeout in 0:30:00.......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "  Success! All tasks reached the 'Completed' state within the specified timeout period.\n",
      "Printing task output...\n",
      "tasks:<azure.batch.models._paged_models.CloudTaskPaged object at 0x7f193a030880>\n",
      "Task: Task0\n",
      "Node: tvmps_f253023ae481e5d91eadca17f5e412fea5d27a7585aaeec15b14ecc95c22460b_d\n",
      "Standard output:\n",
      "﻿With support for Linux, Windows Server, SQL Server, Oracle, IBM, and SAP, Azure Virtual Machines gives you the flexibility of virtualization for a wide range of computing solutions—development and testing, running applications, and extending your datacenter. It’s the freedom of open-source software configured the way you need it. It’s as if it was another rack in your datacenter, giving you the power to deploy an application in minutes instead of weeks.\n",
      "\n",
      "It’s all about choice for your virtual machines. Choose Linux or Windows. Choose to be on-premises, in the cloud, or both. Choose your own virtual machine image or download a certified pre-configured image in our marketplace. With Virtual Machines, you’re in control.\n",
      "\n",
      "Combine the performance of a world-class supercomputer with the scalability of the cloud. Scale from one to thousands of virtual machine instances. Plus, with the growing number of regional Azure datacenters, easily scale globally so you’re closer to where your customers are.\n",
      "\n",
      "Keep your budget in check with low-cost, per-minute billing. You only pay for the compute time you use.\n",
      "\n",
      "We’ll help you encrypt sensitive data, protect virtual machines from viruses and malware, secure network traffic, and meet regulatory and compliance requirements.\n",
      "Task: Task1\n",
      "Node: tvmps_f253023ae481e5d91eadca17f5e412fea5d27a7585aaeec15b14ecc95c22460b_d\n",
      "Standard output:\n",
      "﻿Batch processing began with mainframe computers and punch cards. Today it still plays a central role in business, engineering, science, and other pursuits that require running lots of automated tasks—processing bills and payroll, calculating portfolio risk, designing new products, rendering animated films, testing software, searching for energy, predicting the weather, and finding new cures for disease. Previously only a few had access to the computing power for these scenarios. With Azure Batch, that power is available to you when you need it, without any capital investment.\n",
      "\n",
      "Choose the operating system and development tools you need to run your large-scale jobs on Batch. Batch provides a consistent job scheduling and management experience whether you select Windows Server or Linux compute nodes, but lets you take advantage of the unique features of each environment. With Windows, use your existing Windows-based code, including .NET, to run large-scale compute jobs in Azure. With Linux, choose from popular distributions including CentOS, Ubuntu, and SUSE Linux Enterprise Server to run your compute jobs, or use Docker containers to lift and shift your applications. Batch provides SDKs and supports a range of development tools including Python and Java.\n",
      "\n",
      "Batch runs the applications that you use on workstations and clusters today. It’s easy to cloud-enable your executables and scripts to scale out. Batch provides a queue to receive the work that you want to run and executes your applications. Describe the data that need to be moved to the cloud for processing, how the data should be distributed, what parameters to use for each task, and the command to start the process. Think about this like an assembly line with multiple applications. Batch makes it easy to share data between steps and manage the execution as a whole.\n",
      "\n",
      "You use a workstation today, maybe a small cluster, or you wait in a queue to run your jobs. What if you had access to 16 cores, 100 cores, 10,000 cores, or even 100,000 cores when you needed them, and only had to pay for what you used? With Batch you can. Avoid the bottlenecks and waiting that limit your imagination. What could you do on Azure that you can’t do today?\n",
      "Task: Task2\n",
      "Node: tvmps_f253023ae481e5d91eadca17f5e412fea5d27a7585aaeec15b14ecc95c22460b_d\n",
      "Standard output:\n",
      "﻿Azure Storage offers a set of storage services for all your business needs. Choose from Blob Storage (Object Storage) for unstructured data, File Storage for SMB-based cloud file shares, Table Storage for NoSQL data, Queue Storage to reliably store messages, and Premium Storage for high-performance, low-latency block storage for I/O-intensive workloads running in Azure Virtual Machines.\n",
      "\n",
      "Storage keeps pace with your growing data needs, delivering petabytes of storage for the largest scenarios. Whether you're building modern applications or a high-scale big data application, Storage can handle it.\n",
      "\n",
      "Storage is available in more regions than any other public cloud offering, letting you store your data where it makes the most business sense. Scale up or across data centers as needed, and be closer to your customers for faster access and better performance.\n",
      "\n",
      "Storage automatically replicates your data and maintains multiple copies—either in a single region or globally with geo-redundancy—to help guard against unexpected hardware failures.\n",
      "\n",
      "Deleting container [input]...\n",
      "\n",
      "Sample end: 2020-07-29 00:42:15\n",
      "Elapsed time: 0:18:14\n",
      "\n",
      "Delete job? [Y/n] Y\n",
      "Delete pool? [Y/n] Y\n",
      "\n",
      "Press ENTER to exit...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    start_time = datetime.now().replace(microsecond=0)\n",
    "    print('Sample start: {}'.format(start_time))\n",
    "    print()\n",
    "\n",
    "    # Create the blob client, for use in obtaining references to\n",
    "    # blob storage containers and uploading files to containers.\n",
    "\n",
    "    blob_client = azureblob.BlockBlobService(\n",
    "        account_name=config._STORAGE_ACCOUNT_NAME,\n",
    "        account_key=config._STORAGE_ACCOUNT_KEY)\n",
    "\n",
    "    # Use the blob client to create the containers in Azure Storage if they\n",
    "    # don't yet exist.\n",
    "\n",
    "    input_container_name = 'input'\n",
    "    blob_client.create_container(input_container_name, fail_on_exist=False)\n",
    "\n",
    "    # The collection of data files that are to be processed by the tasks.\n",
    "    input_file_paths = [os.path.join(sys.path[0], 'taskdata0.txt'),\n",
    "                        os.path.join(sys.path[0], 'taskdata1.txt'),\n",
    "                        os.path.join(sys.path[0], 'taskdata2.txt')]\n",
    "\n",
    "    # Upload the data files.\n",
    "    input_files = [\n",
    "        upload_file_to_container(blob_client, input_container_name, file_path)\n",
    "        for file_path in input_file_paths]\n",
    "\n",
    "    # Create a Batch service client. We'll now be interacting with the Batch\n",
    "    # service in addition to Storage\n",
    "    credentials = batch_auth.SharedKeyCredentials(config._BATCH_ACCOUNT_NAME,\n",
    "                                                  config._BATCH_ACCOUNT_KEY)\n",
    "\n",
    "    batch_client = batch.BatchServiceClient(\n",
    "        credentials,\n",
    "        batch_url=config._BATCH_ACCOUNT_URL)\n",
    "\n",
    "    try:\n",
    "        # Create the pool that will contain the compute nodes that will execute the\n",
    "        # tasks.\n",
    "        create_pool(batch_client, config._POOL_ID)\n",
    "        time.sleep(15)\n",
    "        \n",
    "        # Starting Autoscaling \n",
    "        formula = '''// This is another example that adjusts the pool size based on the number of tasks. This formula also takes into account the MaxTasksPerComputeNode value that has been set for the pool. This is particularly useful in situations where parallel task execution has been enabled on your pool.\n",
    "\n",
    "        // Determine whether 70 percent of the samples have been recorded in the past 15 minutes; if not, use last sample\n",
    "        percentage = 70;\n",
    "        span = TimeInterval_Minute * 15;\n",
    "        $samples = $ActiveTasks.GetSamplePercent(span);\n",
    "        $tasks = $samples < percentage ? max(0,$ActiveTasks.GetSample(1)) : max( $ActiveTasks.GetSample(1), avg($ActiveTasks.GetSample(span)));\n",
    "        // Set the number of nodes to add to one-fourth the number of active tasks (the MaxTasksPerComputeNode property on this pool is set to 4, adjust this number for your use case)\n",
    "        multiplier = 0.25;\n",
    "        $cores = $TargetDedicatedNodes * 4;\n",
    "        $extraVMs = (($tasks - $cores) + 3) * multiplier;\n",
    "        $targetVMs = ($TargetDedicatedNodes + $extraVMs);\n",
    "        // Attempt to grow the number of compute nodes to match the number of active tasks, with a maximum of 3\n",
    "        $TargetDedicatedNodes = max(0, min($targetVMs, 3));\n",
    "        // Keep the nodes active until the tasks finish\n",
    "        $NodeDeallocationOption = taskcompletion;'''\n",
    "        \n",
    "        response1 = batch_client.pool.enable_auto_scale(pool_id=config._POOL_ID,auto_scale_formula=formula,\n",
    "                                           auto_scale_evaluation_interval=timedelta(minutes=15))\n",
    "\n",
    "        # Create the job that will run the tasks.\n",
    "        create_job(batch_client, config._JOB_ID, config._POOL_ID)\n",
    "\n",
    "        # Add the tasks to the job.\n",
    "        add_tasks(batch_client, config._JOB_ID, input_files)\n",
    "\n",
    "        # Pause execution until tasks reach Completed state. \n",
    "        wait_for_tasks_to_complete(batch_client, config._JOB_ID, timedelta(minutes=30))\n",
    "\n",
    "        print(\"  Success! All tasks reached the 'Completed' state within the \"\n",
    "              \"specified timeout period.\")\n",
    "\n",
    "        # Print the stdout.txt and stderr.txt files for each task to the console\n",
    "        print_task_output(batch_client, config._JOB_ID)\n",
    "\n",
    "    except batchmodels.BatchErrorException as err:\n",
    "        print_batch_exception(err)\n",
    "        raise\n",
    "\n",
    "    # Clean up storage resources\n",
    "    print('Deleting container [{}]...'.format(input_container_name))\n",
    "    blob_client.delete_container(input_container_name)\n",
    "\n",
    "    # Print out some timing info\n",
    "    end_time = datetime.now().replace(microsecond=0)\n",
    "    print()\n",
    "    print('Sample end: {}'.format(end_time))\n",
    "    print('Elapsed time: {}'.format(end_time - start_time))\n",
    "    print()\n",
    "\n",
    "    # Clean up Batch resources (if the user so chooses).\n",
    "    if query_yes_no('Delete job?') == 'yes':\n",
    "        batch_client.job.delete(config._JOB_ID)\n",
    "\n",
    "    if query_yes_no('Delete pool?') == 'yes':\n",
    "        batch_client.pool.delete(config._POOL_ID)\n",
    "\n",
    "    print()\n",
    "    input('Press ENTER to exit...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
